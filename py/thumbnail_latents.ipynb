{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from json import JSONDecodeError\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from util.constants import Topic, ThumbnailURL, thumbnail_URL\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class ImageLatentRepresentationModel(ViTForImageClassification):\n",
    "    \"\"\"\n",
    "    Hook into the ViTForImageClassification Class in order to get the latent\n",
    "    representations of an image, not just the classification output. Source code\n",
    "    taken from HuggingFace open-source GitHub:\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_vit.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"\n",
    "        Overwritten forward method to only get latent representation of the image,\n",
    "        without image classification.\n",
    "\n",
    "        args:\n",
    "            - pixel_values: input image, as PyTorch Tensor of shape [1,3,224,224] \n",
    "        \n",
    "        returns:\n",
    "            - latent_vec: latent vector representing the image\n",
    "        \"\"\"\n",
    "        vit_output = self.vit(pixel_values)\n",
    "        latent_vec = vit_output[0][:,0,:]\n",
    "\n",
    "        return latent_vec\n",
    "\n",
    "def load_model(device, install=True):\n",
    "    \"\"\"\n",
    "    Loads in a pre-trained ViT model for latent image representation\n",
    "\n",
    "    args:\n",
    "        - device: what device the model should be on\n",
    "    returns:\n",
    "        - model: pre-trained ViT model\n",
    "        - feature_extractor: pre-trained feature extractor for processing images\n",
    "    \"\"\"\n",
    "    if install:\n",
    "        print(\"Installing ViT architecture... This may take a couple of minutes.\")\n",
    "        os.system(\"pip install -q git+https://github.com/huggingface/transformers.git\")\n",
    "        print(\"Finished installing.\")\n",
    "\n",
    "    print(\"Loading pretrained ViT model...\")\n",
    "    model = ImageLatentRepresentationModel.from_pretrained('google/vit-base-patch16-224')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    print(\"Loaded model\")\n",
    "\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    return model, feature_extractor\n",
    "\n",
    "def get_latent_vectors(model, ft_extr, raw_imgs, device):\n",
    "    \"\"\"\n",
    "    Function to get the latent representation of an image.\n",
    "\n",
    "    args:\n",
    "        - model: The pretrained ViT model\n",
    "        - ft_extr: The feature extractor, used to preprocess the images\n",
    "        - raw_imgs: The raw thumbnail images\n",
    "    \"\"\"\n",
    "    # Encode the images using the feature extractor\n",
    "    encodings = ft_extr(images=raw_imgs, return_tensors=\"pt\")\n",
    "    pixel_values = encodings['pixel_values'].to(device)\n",
    "\n",
    "    # Get the latent representation by passing it through the network\n",
    "    latent_vecs = model(pixel_values)\n",
    "    del pixel_values\n",
    "    return latent_vecs.detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get latents for all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "channel_videos_dict = {}\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        channel_videos_dict.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(\"..\", \"data\", \"thumbnail-latents\")\n",
    "\n",
    "video_results_dir = os.path.join(\"..\",\"..\",\"DATA\",\"thumbnail-latents\",\"videos\")\n",
    "channel_results_dir = os.path.join(RESULTS_DIR, \"channels\")\n",
    "categories_results_dir = os.path.join(RESULTS_DIR, \"categories\")\n",
    "\n",
    "if not os.path.exists(video_results_dir):\n",
    "    os.makedirs(video_results_dir)\n",
    "\n",
    "if not os.path.exists(categories_results_dir):\n",
    "    os.makedirs(categories_results_dir)\n",
    "\n",
    "def get_done_list(dir):\n",
    "    return [nm.replace(\".json\",'').replace('.pt','') for nm in os.listdir(dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code in batches\n",
    "THUMBNAIL_DIR = os.path.join(\"..\",\"..\",\"thumbnails\")\n",
    "quality = ThumbnailURL.high\n",
    "\n",
    "FETCH_THUMBS = False\n",
    "if not FETCH_THUMBS:\n",
    "    available_ids = [nm.replace('_high.jpg','') for nm in get_done_list(THUMBNAIL_DIR)]\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "model, feature_extractor = load_model(device, install=False)\n",
    "\n",
    "for channel,videos in tqdm(channel_videos_dict.items()):\n",
    "    done_path = os.path.join(RESULTS_DIR, \"videos_done\", channel+\".json\")\n",
    "    ids = [vid[\"id\"] for vid in videos[:30]]\n",
    "    if not FETCH_THUMBS:\n",
    "        ids = list(set(ids).intersection(available_ids))\n",
    "        if not ids:\n",
    "            continue\n",
    "    try:\n",
    "        with open(done_path, \"r\") as f:\n",
    "            done_ids = json.load(f)\n",
    "        if set(ids).issubset(done_ids): # Every id is done\n",
    "            continue\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    fetched_ids = []\n",
    "    results = []\n",
    "    for i in range(2):\n",
    "        id_batch = ids[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        if FETCH_THUMBS:\n",
    "            raws = []\n",
    "            for id in id_batch:\n",
    "                url = thumbnail_URL(id, quality)\n",
    "                try:\n",
    "                    raws.append(requests.get(url, stream=True).raw)\n",
    "                except:\n",
    "                    continue\n",
    "                fetched_ids.append(id)\n",
    "            images = [np.array(Image.open(raw)) for raw in raws]\n",
    "        else:\n",
    "            imgs_paths = [os.path.join(THUMBNAIL_DIR, id+\"_high.jpg\") for id in id_batch]\n",
    "            fetched_ids.extend(id_batch)\n",
    "            images = [Image.open(path) for path in imgs_paths]\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            results.extend(get_latent_vectors(model, feature_extractor, images, device))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    ids = fetched_ids\n",
    "\n",
    "    path = os.path.join(video_results_dir, f\"{channel}.pt\")\n",
    "    torch.save(np.array(results), path)\n",
    "    with open(done_path, \"w\") as f:\n",
    "        json.dump(ids, f)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "channel_videos_dict = {}\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        channel_videos_dict.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate channel results\n",
    "for channel,videos in tqdm(channel_videos_dict.items()):\n",
    "    in_path = os.path.join(video_results_dir,channel+\".pt\")\n",
    "    try:\n",
    "        result_list = torch.load(in_path)\n",
    "    except:\n",
    "        print(f\"couldn't load {channel}\")\n",
    "\n",
    "    channel_mean = result_list.mean(axis=0)\n",
    "    channel_result = {\n",
    "        \"std\": float(result_list.std()),\n",
    "        \"len\": len(result_list),\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(channel_results_dir, f\"{channel}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(channel_result, f)\n",
    "    filepath = os.path.join(RESULTS_DIR, \"channels_mean\", f\"{channel}.pt\")\n",
    "    torch.save(channel_mean, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(os.path.join(\"..\", \"data\", \"channel2category.json\"), \"r\") as f:\n",
    "    channel2cat = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of results per channel for each category\n",
    "category_results_list = defaultdict(list)\n",
    "for channel in tqdm(get_done_list(channel_results_dir)):\n",
    "    cat = channel2cat[channel]\n",
    "    results = {}\n",
    "    for folder,ext in [(\"channels\",\".json\"), (\"channels_mean\",\".pt\")]:\n",
    "        filepath = os.path.join(RESULTS_DIR, folder, channel+ext)\n",
    "        try:\n",
    "            if ext == \".json\":\n",
    "                with open(filepath, \"r\") as f:\n",
    "                    results.update(json.load(f))\n",
    "            elif ext == \".pt\":\n",
    "                results.update({\"mean\": torch.load(filepath)})\n",
    "        except JSONDecodeError:\n",
    "            print(f\"couldn't open {channel}; deleting file\")\n",
    "            os.remove(filepath)\n",
    "    category_results_list[cat].append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate category results\n",
    "for cat,stats_list in category_results_list.items():\n",
    "    mean_list = np.array([channel_stats[\"mean\"] for channel_stats in stats_list if channel_stats[\"mean\"].shape])\n",
    "    std_list = np.array([channel_stats[\"std\"] for channel_stats in stats_list])\n",
    "    category_mean = mean_list.mean(axis=0)\n",
    "    category_result = {\n",
    "        \"std\": float(mean_list.std()),\n",
    "        \"avg_std_of_channels\": std_list.mean(),\n",
    "        \"len\": len(mean_list),\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(RESULTS_DIR, \"categories\", f\"{cat}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(category_result, f)\n",
    "    filepath = os.path.join(RESULTS_DIR, \"categories_mean\", f\"{cat}.pt\")\n",
    "    torch.save(category_mean, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = get_done_list(channel_results_dir)\n",
    "\n",
    "channel_means = []\n",
    "for channel in tqdm(channels):\n",
    "    path = os.path.join(RESULTS_DIR, \"channels_mean\", channel+\".pt\")\n",
    "    channel_mean = torch.load(path)\n",
    "    if not channel_mean.shape:\n",
    "        continue\n",
    "    channel_means.append(channel_mean)\n",
    "channel_means = torch.Tensor(np.array(channel_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = Topic._member_names_\n",
    "\n",
    "category_means = []\n",
    "for cat in tqdm(categories):\n",
    "    path = os.path.join(RESULTS_DIR, \"categories_mean\", cat+\".pt\")\n",
    "    category_means.append(torch.load(path))\n",
    "category_means = torch.Tensor(category_means)\n",
    "\n",
    "all_means = torch.cat((channel_means, category_means), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(arrayA, arrayB):\n",
    "    \"\"\"[N,D], [M,D] -> [N,M]\"\"\"\n",
    "    normA = torch.norm(arrayA, p=2, dim=1)[:,None]\n",
    "    normB = torch.norm(arrayB, p=2, dim=1)[:,None]\n",
    "    return arrayA/normA @ (arrayB/normB).T\n",
    "    \n",
    "def dist(tensorA, tensorB):\n",
    "    \"\"\"[N,D], [M,D] -> [N,M]\"\"\"\n",
    "    return ((tensorA[:,:,None] - tensorB.transpose()[None,:,:])**2).mean(1)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "all_means = all_means.to(device)\n",
    "best_similarity = torch.zeros(len(all_means)).to(device)\n",
    "best_repr = [\"\"]*len(all_means)\n",
    "\n",
    "batch_size = 128\n",
    "for i in tqdm(range(len(channels)//batch_size+1)):\n",
    "    done_ids = []\n",
    "    done_means = torch.zeros((0,768), device=device)\n",
    "    for channel in channels[i*batch_size:(i+1)*batch_size]:\n",
    "        done_path = os.path.join(RESULTS_DIR, \"videos_done\", channel+\".json\")\n",
    "        try:\n",
    "            with open(done_path, \"r\") as f:\n",
    "                done_ids.extend(json.load(f))\n",
    "            if not done_ids:\n",
    "                continue\n",
    "        except FileNotFoundError:\n",
    "            print(f\"couldn't open {channel}\")\n",
    "            continue\n",
    "        \n",
    "        filepath = os.path.join(video_results_dir, channel+\".pt\")\n",
    "        loaded_means = torch.Tensor(torch.load(filepath)).to(device)\n",
    "        done_means = torch.cat((done_means, loaded_means), dim=0)\n",
    "\n",
    "    if len(done_means) < 1:\n",
    "        continue\n",
    "\n",
    "    similarities = torch.max(cos_sim(all_means, done_means), dim=-1)\n",
    "    update_best = (similarities.values > best_similarity).nonzero()\n",
    "    best_similarity[update_best] = similarities.values[update_best]\n",
    "    for idx in update_best:\n",
    "        best_repr[idx] = done_ids[similarities.indices[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_best_repr = {k:{\"sim\": sim.item(), \"vid_id\": id} for k,sim,id in zip(channels,best_similarity,best_repr) if id}\n",
    "with open(os.path.join(RESULTS_DIR, \"channels_best_repr.json\"), \"w\") as f:\n",
    "    json.dump(channels_best_repr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_best_repr = {k:{\"sim\": sim.item(), \"vid_id\": id} for k,sim,id in zip(categories,best_similarity[-9:],best_repr[-9:]) if id}\n",
    "with open(os.path.join(RESULTS_DIR, \"categories_best_repr.json\"), \"w\") as f:\n",
    "    json.dump(categories_best_repr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video latent deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = get_done_list(channel_results_dir)\n",
    "\n",
    "channel2mean = {}\n",
    "for channel in tqdm(channels):\n",
    "    path = os.path.join(RESULTS_DIR, \"channels_mean\", channel+\".pt\")\n",
    "    channel2mean[channel] = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid2views = {}\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        videos_info = json.load(f)\n",
    "    vid2views.update({vid[\"id\"]:vid[\"views\"] for channel_vids in tqdm(videos_info.values()) for vid in channel_vids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(channels):\n",
    "    done_path = os.path.join(RESULTS_DIR, \"videos_done\", channel+\".json\")\n",
    "    try:\n",
    "        with open(done_path, \"r\") as f:\n",
    "            done_ids = json.load(f)\n",
    "        if not done_ids:\n",
    "            continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"couldn't open {channel}\")\n",
    "        continue\n",
    "    \n",
    "    filepath = os.path.join(video_results_dir, channel+\".pt\")\n",
    "    loaded_means = torch.load(filepath)\n",
    "    done_means = loaded_means\n",
    "\n",
    "    if len(done_means) < 1:\n",
    "        continue\n",
    "\n",
    "    distances = dist(channel2mean[channel][None], done_means)[0]\n",
    "\n",
    "    result = {\n",
    "        \"mean\": float(distances.mean()),\n",
    "        \"datapoints\": [{\n",
    "            \"x\": float(d),\n",
    "            \"y\": vid2views[id],\n",
    "            \"id\": id,\n",
    "        } for d,id in zip(distances, done_ids)]\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"video_deviation\", channel+\".json\"), \"w\") as f:\n",
    "        json.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba6cb22e6e13b5daf99d66a8b604561524e31a3f4c87653eefde1e44b7a8fce3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
