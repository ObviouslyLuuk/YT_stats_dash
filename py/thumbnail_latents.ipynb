{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from json import JSONDecodeError\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from util.constants import Topic, ThumbnailURL, thumbnail_URL\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class ImageLatentRepresentationModel(ViTForImageClassification):\n",
    "    \"\"\"\n",
    "    Hook into the ViTForImageClassification Class in order to get the latent\n",
    "    representations of an image, not just the classification output. Source code\n",
    "    taken from HuggingFace open-source GitHub:\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_vit.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"\n",
    "        Overwritten forward method to only get latent representation of the image,\n",
    "        without image classification.\n",
    "\n",
    "        args:\n",
    "            - pixel_values: input image, as PyTorch Tensor of shape [1,3,224,224] \n",
    "        \n",
    "        returns:\n",
    "            - latent_vec: latent vector representing the image\n",
    "        \"\"\"\n",
    "        vit_output = self.vit(pixel_values)\n",
    "        latent_vec = vit_output[0][:,0,:]\n",
    "\n",
    "        return latent_vec\n",
    "\n",
    "def load_model(device, install=True):\n",
    "    \"\"\"\n",
    "    Loads in a pre-trained ViT model for latent image representation\n",
    "\n",
    "    args:\n",
    "        - device: what device the model should be on\n",
    "    returns:\n",
    "        - model: pre-trained ViT model\n",
    "        - feature_extractor: pre-trained feature extractor for processing images\n",
    "    \"\"\"\n",
    "    if install:\n",
    "        print(\"Installing ViT architecture... This may take a couple of minutes.\")\n",
    "        os.system(\"pip install -q git+https://github.com/huggingface/transformers.git\")\n",
    "        print(\"Finished installing.\")\n",
    "\n",
    "    print(\"Loading pretrained ViT model...\")\n",
    "    model = ImageLatentRepresentationModel.from_pretrained('google/vit-base-patch16-224')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    print(\"Loaded model\")\n",
    "\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    return model, feature_extractor\n",
    "\n",
    "def get_latent_vectors(model, ft_extr, raw_imgs, device):\n",
    "    \"\"\"\n",
    "    Function to get the latent representation of an image.\n",
    "\n",
    "    args:\n",
    "        - model: The pretrained ViT model\n",
    "        - ft_extr: The feature extractor, used to preprocess the images\n",
    "        - raw_imgs: The raw thumbnail images\n",
    "    \"\"\"\n",
    "    # Encode the images using the feature extractor\n",
    "    encodings = ft_extr(images=raw_imgs, return_tensors=\"pt\")\n",
    "    pixel_values = encodings['pixel_values'].to(device)\n",
    "#     print(f\"Size of images in memory: {pixel_values.element_size()*pixel_values.nelement()} bytes\")\n",
    "\n",
    "    # Get the latent representation by passing it through the network\n",
    "    latent_vecs = model(pixel_values)\n",
    "    del pixel_values\n",
    "    return latent_vecs\n",
    "\n",
    "def generate_repr_stats(model, feature_extractor, video_thumbnails):\n",
    "    \"\"\"\n",
    "    Function to generate all thumbnail latent representation statistics.\n",
    "\n",
    "    args:\n",
    "        - model: pre-trained ViT model\n",
    "        - feature_extractor: pre-trained feature extractor for processing images\n",
    "        - video_thumbnails: images we want to represent\n",
    "    \"\"\"\n",
    "    latents = get_latent_vectors(model, feature_extractor, video_thumbnails, device)\n",
    "    latents_save = latents.detach().cpu().numpy().tolist()\n",
    "    del latents\n",
    "    return latents_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get latents for all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "videos = []\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        videos_info = json.load(f)\n",
    "        videos.extend([vid for channel_vids in videos_info.values() for vid in channel_vids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(\"..\", \"data\", \"thumbnail-latents\")\n",
    "\n",
    "video_results_dir = os.path.join(\"..\",\"..\",\"DATA\",\"thumbnail-latents\",\"videos\")\n",
    "channel_results_dir = os.path.join(RESULTS_DIR, \"channels\")\n",
    "categories_results_dir = os.path.join(RESULTS_DIR, \"categories\")\n",
    "\n",
    "if not os.path.exists(video_results_dir):\n",
    "    os.makedirs(video_results_dir)\n",
    "\n",
    "if not os.path.exists(categories_results_dir):\n",
    "    os.makedirs(categories_results_dir)\n",
    "\n",
    "def get_done_list(dir):\n",
    "    return [nm.replace(\".json\",'').replace('.pt','') for nm in os.listdir(dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code in batches\n",
    "done_list = get_done_list(video_results_dir)\n",
    "all_ids = [vid[\"id\"] for vid in tqdm(videos)]\n",
    "todo_ids = list(set(all_ids).difference(done_list))\n",
    "\n",
    "THUMBNAIL_DIR = os.path.join(\"..\",\"data\",\"thumbnails\")\n",
    "quality = ThumbnailURL.high\n",
    "\n",
    "FETCH_THUMBS = True\n",
    "if not FETCH_THUMBS:\n",
    "    available_ids = [nm.replace('_high.jpg','') for nm in get_done_list(THUMBNAIL_DIR)]\n",
    "    todo_ids = list(set(todo_ids).intersection(available_ids))\n",
    "\n",
    "batch_size = 16\n",
    "batch_num = len(todo_ids)//batch_size\n",
    "if batch_num != int(len(todo_ids)/batch_size):\n",
    "    batch_num += 1\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "model, feature_extractor = load_model(device, install=False)\n",
    "\n",
    "for batch in tqdm(range(batch_num)):\n",
    "    ids = todo_ids[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "    if FETCH_THUMBS:\n",
    "        raws = []\n",
    "        fetched_ids = []\n",
    "        for id in ids:\n",
    "            url = thumbnail_URL(id, quality)\n",
    "            try:\n",
    "                raws.append(requests.get(url, stream=True).raw)\n",
    "            except:\n",
    "                continue\n",
    "            fetched_ids.append(id)\n",
    "        images = [Image.open(raw) for raw in raws]\n",
    "        ids = fetched_ids\n",
    "    else:\n",
    "        imgs_paths = [os.path.join(THUMBNAIL_DIR, id+\"_high.jpg\") for id in ids]\n",
    "        images = [Image.open(path) for path in imgs_paths]\n",
    "\n",
    "    batch_latents = generate_repr_stats(model, feature_extractor, images)\n",
    "\n",
    "    for vid_id, result in zip(ids, batch_latents):\n",
    "        path = os.path.join(video_results_dir, f\"{vid_id}.pt\")\n",
    "        torch.save(result, path)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(os.path.join(\"..\", \"data\", \"vid2channel.json\"), \"r\") as f:\n",
    "    vid2channel = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of results per video for each channel\n",
    "channel_result_list = defaultdict(list)\n",
    "for vid_id in tqdm(get_done_list(video_results_dir)):\n",
    "    channel = vid2channel[vid_id]\n",
    "    filepath = os.path.join(video_results_dir, f\"{vid_id}.json\")\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            result = json.load(f)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"couldn't open {vid_id}; deleting file\")\n",
    "        os.remove(filepath)\n",
    "    channel_result_list[channel].append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate channel results\n",
    "for channel,result_list in channel_result_list.items():\n",
    "    result_list = np.array(result_list)\n",
    "    channel_result = {\n",
    "        \"mean\": result_list.mean(axis=0).tolist(),\n",
    "        \"std\": result_list.std(),\n",
    "        \"len\": len(result_list),\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(channel_results_dir, f\"{channel}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(channel_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(os.path.join(\"..\", \"data\", \"channel2category.json\"), \"r\") as f:\n",
    "    channel2cat = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of results per channel for each category\n",
    "category_results_list = defaultdict(list)\n",
    "for channel in tqdm(get_done_list(channel_results_dir)):\n",
    "    cat = channel2cat[channel]\n",
    "    filepath = os.path.join(channel_results_dir, f\"{channel}.json\")\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"couldn't open {channel}; deleting file\")\n",
    "        os.remove(filepath)\n",
    "    category_results_list[cat].append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate category results\n",
    "for cat,stats_list in category_results_list.items():\n",
    "    mean_list = np.array([channel_stats[\"mean\"] for channel_stats in stats_list])\n",
    "    std_list = np.array([channel_stats[\"std\"] for channel_stats in stats_list])\n",
    "    category_result = {\n",
    "        \"mean\": mean_list.mean(axis=0).tolist(),\n",
    "        \"std\": std_list.mean(),\n",
    "        \"len\": len(mean_list),\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(RESULTS_DIR, \"categories\", f\"{cat}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(category_result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b74cd7db1eb9f8499da7dbef20678a005a07ab79df7dd49707a224686fb33242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
