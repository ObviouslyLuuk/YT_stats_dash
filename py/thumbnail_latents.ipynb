{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from json import JSONDecodeError\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from util.constants import Topic, ThumbnailURL, thumbnail_URL\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class ImageLatentRepresentationModel(ViTForImageClassification):\n",
    "    \"\"\"\n",
    "    Hook into the ViTForImageClassification Class in order to get the latent\n",
    "    representations of an image, not just the classification output. Source code\n",
    "    taken from HuggingFace open-source GitHub:\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_vit.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"\n",
    "        Overwritten forward method to only get latent representation of the image,\n",
    "        without image classification.\n",
    "\n",
    "        args:\n",
    "            - pixel_values: input image, as PyTorch Tensor of shape [1,3,224,224] \n",
    "        \n",
    "        returns:\n",
    "            - latent_vec: latent vector representing the image\n",
    "        \"\"\"\n",
    "        vit_output = self.vit(pixel_values)\n",
    "        latent_vec = vit_output[0][:,0,:]\n",
    "\n",
    "        return latent_vec\n",
    "\n",
    "def load_model(device, install=True):\n",
    "    \"\"\"\n",
    "    Loads in a pre-trained ViT model for latent image representation\n",
    "\n",
    "    args:\n",
    "        - device: what device the model should be on\n",
    "    returns:\n",
    "        - model: pre-trained ViT model\n",
    "        - feature_extractor: pre-trained feature extractor for processing images\n",
    "    \"\"\"\n",
    "    if install:\n",
    "        print(\"Installing ViT architecture... This may take a couple of minutes.\")\n",
    "        os.system(\"pip install -q git+https://github.com/huggingface/transformers.git\")\n",
    "        print(\"Finished installing.\")\n",
    "\n",
    "    print(\"Loading pretrained ViT model...\")\n",
    "    model = ImageLatentRepresentationModel.from_pretrained('google/vit-base-patch16-224')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    print(\"Loaded model\")\n",
    "\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    return model, feature_extractor\n",
    "\n",
    "def get_latent_vectors(model, ft_extr, raw_imgs, device):\n",
    "    \"\"\"\n",
    "    Function to get the latent representation of an image.\n",
    "\n",
    "    args:\n",
    "        - model: The pretrained ViT model\n",
    "        - ft_extr: The feature extractor, used to preprocess the images\n",
    "        - raw_imgs: The raw thumbnail images\n",
    "    \"\"\"\n",
    "    # Encode the images using the feature extractor\n",
    "    encodings = ft_extr(images=raw_imgs, return_tensors=\"pt\")\n",
    "    pixel_values = encodings['pixel_values'].to(device)\n",
    "\n",
    "    # Get the latent representation by passing it through the network\n",
    "    latent_vecs = model(pixel_values)\n",
    "    del pixel_values\n",
    "    return latent_vecs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get latents for all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "channel_videos_dict = {}\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        channel_videos_dict.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(\"..\", \"data\", \"thumbnail-latents\")\n",
    "\n",
    "video_results_dir = os.path.join(\"..\",\"..\",\"DATA\",\"thumbnail-latents\",\"videos\")\n",
    "channel_results_dir = os.path.join(RESULTS_DIR, \"channels\")\n",
    "categories_results_dir = os.path.join(RESULTS_DIR, \"categories\")\n",
    "\n",
    "if not os.path.exists(video_results_dir):\n",
    "    os.makedirs(video_results_dir)\n",
    "\n",
    "if not os.path.exists(categories_results_dir):\n",
    "    os.makedirs(categories_results_dir)\n",
    "\n",
    "def get_done_list(dir):\n",
    "    return [nm.replace(\".json\",'').replace('.pt','') for nm in os.listdir(dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code in batches\n",
    "THUMBNAIL_DIR = os.path.join(\"..\",\"data\",\"thumbnails\")\n",
    "quality = ThumbnailURL.high\n",
    "\n",
    "FETCH_THUMBS = True\n",
    "if not FETCH_THUMBS:\n",
    "    available_ids = [nm.replace('_high.jpg','') for nm in get_done_list(THUMBNAIL_DIR)]\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "model, feature_extractor = load_model(device, install=False)\n",
    "\n",
    "for channel,videos in tqdm(channel_videos_dict.items()):\n",
    "    done_path = os.path.join(RESULTS_DIR, \"videos_done\", channel+\".json\")\n",
    "    ids = [vid[\"id\"] for vid in videos[:30]]\n",
    "    try:\n",
    "        with open(done_path, \"r\") as f:\n",
    "            done_ids = json.load(f)\n",
    "        if set(ids).issubset(done_ids): # Every id is done\n",
    "            continue\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    fetched_ids = []\n",
    "    results = []\n",
    "    for i in range(2):\n",
    "        id_batch = ids[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        if FETCH_THUMBS:\n",
    "            raws = []\n",
    "            for id in id_batch:\n",
    "                url = thumbnail_URL(id, quality)\n",
    "                try:\n",
    "                    raws.append(requests.get(url, stream=True).raw)\n",
    "                except:\n",
    "                    continue\n",
    "                fetched_ids.append(id)\n",
    "            images = [Image.open(raw) for raw in raws]\n",
    "            ids = fetched_ids\n",
    "        else:\n",
    "            imgs_paths = [os.path.join(THUMBNAIL_DIR, id+\"_high.jpg\") for id in ids]\n",
    "            images = [Image.open(path) for path in imgs_paths]\n",
    "\n",
    "        results.extend(get_latent_vectors(model, feature_extractor, images, device))\n",
    "\n",
    "    path = os.path.join(video_results_dir, f\"{channel}.pt\")\n",
    "    torch.save(np.array(results), path)\n",
    "    with open(done_path, \"w\") as f:\n",
    "        json.dump(fetched_ids, f)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(os.path.join(\"..\", \"data\", \"vid2channel.json\"), \"r\") as f:\n",
    "    vid2channel = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of results per video for each channel\n",
    "channel_result_list = defaultdict(list)\n",
    "for vid_id in tqdm(get_done_list(video_results_dir)):\n",
    "    channel = vid2channel[vid_id]\n",
    "    filepath = os.path.join(video_results_dir, f\"{vid_id}.json\")\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            result = json.load(f)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"couldn't open {vid_id}; deleting file\")\n",
    "        os.remove(filepath)\n",
    "    channel_result_list[channel].append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate channel results\n",
    "for channel,result_list in channel_result_list.items():\n",
    "    result_list = np.array(result_list)\n",
    "    channel_result = {\n",
    "        \"mean\": result_list.mean(axis=0).tolist(),\n",
    "        \"std\": result_list.std(),\n",
    "        \"len\": len(result_list),\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(channel_results_dir, f\"{channel}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(channel_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(os.path.join(\"..\", \"data\", \"channel2category.json\"), \"r\") as f:\n",
    "    channel2cat = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of results per channel for each category\n",
    "category_results_list = defaultdict(list)\n",
    "for channel in tqdm(get_done_list(channel_results_dir)):\n",
    "    cat = channel2cat[channel]\n",
    "    filepath = os.path.join(channel_results_dir, f\"{channel}.json\")\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    except JSONDecodeError:\n",
    "        print(f\"couldn't open {channel}; deleting file\")\n",
    "        os.remove(filepath)\n",
    "    category_results_list[cat].append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate category results\n",
    "for cat,stats_list in category_results_list.items():\n",
    "    mean_list = np.array([channel_stats[\"mean\"] for channel_stats in stats_list])\n",
    "    std_list = np.array([channel_stats[\"std\"] for channel_stats in stats_list])\n",
    "    category_result = {\n",
    "        \"mean\": mean_list.mean(axis=0).tolist(),\n",
    "        \"std\": std_list.mean(),\n",
    "        \"len\": len(mean_list),\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(RESULTS_DIR, \"categories\", f\"{cat}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(category_result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b74cd7db1eb9f8499da7dbef20678a005a07ab79df7dd49707a224686fb33242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
