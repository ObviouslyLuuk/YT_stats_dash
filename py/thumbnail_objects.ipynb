{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from util.constants import Topic, ThumbnailURL, thumbnail_URL\n",
    "import requests\n",
    "\n",
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def sum_dicts(dics):\n",
    "    new_dic = defaultdict(int)\n",
    "    for dic in dics:\n",
    "        for k,v in dic.items():\n",
    "            new_dic[k] += v\n",
    "    return new_dic\n",
    "\n",
    "def extend_dicts(dics):\n",
    "    new_dic = defaultdict(list)\n",
    "    for dic in dics:\n",
    "        for k,l in dic.items():\n",
    "            new_dic[k].extend(l)\n",
    "    return new_dic\n",
    "\n",
    "def sort_dict(dic):\n",
    "    return {k:v for k,v in sorted(dic.items(), key=lambda x: x[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"coco_classes.txt\"), \"r\") as f:\n",
    "    coco_classes = [c.rstrip(\"\\n\") for c in f.readlines()]\n",
    "coco_classes.insert(0, \"unknown\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "feature_extractor = YolosFeatureExtractor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "def load_model():\n",
    "    return YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\").to(device)\n",
    "YOLOS_model = load_model()\n",
    "\n",
    "# Print model size\n",
    "mem_params = sum([param.nelement()*param.element_size() for param in YOLOS_model.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in YOLOS_model.buffers()])\n",
    "print(f\"Memory used by model: {mem_params + mem_bufs} bytes\")\n",
    "\n",
    "\n",
    "def model(images):\n",
    "    inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
    "    outputs = YOLOS_model(**inputs)\n",
    "\n",
    "    ls = np.zeros((len(images),0)).tolist()\n",
    "\n",
    "    # model predicts bounding boxes and corresponding COCO classes\n",
    "    logits = outputs.logits.detach().cpu()\n",
    "    bboxes = outputs.pred_boxes.detach().cpu()\n",
    "\n",
    "    probs = F.softmax(logits.clone(), dim=-1)  # [B, 100, 92]\n",
    "    preds      = probs.argmax(-1) # [B, 100]\n",
    "    confidence = probs.max(-1)[0] # [B, 100]\n",
    "    known_indices     = [(preds_img != 91).nonzero()[:,0] for preds_img in preds] # [B, known indices]\n",
    "    confident_indices = [(conf_img > 0.75).nonzero()[:,0] for conf_img in confidence] # [B, confident indices]\n",
    "    indices = [list(set(known_idx_img.tolist()).intersection(set(conf_idx_img.tolist()))) for known_idx_img,conf_idx_img in zip(known_indices, confident_indices)] # [B, intersection of indices]\n",
    "    pred_classes = [[coco_classes[v] for v in preds_img[idx_img]] for preds_img, idx_img in zip(preds, indices)] # [B, predicted classes]\n",
    "\n",
    "    confidence = np.round((confidence*100).tolist(), 1)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        for j, patch_idx in enumerate(indices[i]):\n",
    "            c = pred_classes[i][j]\n",
    "            conf = confidence[i][patch_idx]\n",
    "            bbox = np.round(bboxes[i, patch_idx].tolist(), 3).tolist()\n",
    "\n",
    "            ls[i].append({\n",
    "                \"pred_class\": c,\n",
    "                \"conf\": conf,\n",
    "                \"bbox\": bbox\n",
    "            })\n",
    "    \n",
    "    return ls\n",
    "\n",
    "def resize(img, new_width=100):\n",
    "    wpercent = (new_width/float(img.size[0]))\n",
    "    hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    return img.resize((new_width,hsize), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get objects for all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "videos = []\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        videos_info = json.load(f)\n",
    "        videos.extend([vid for channel_vids in videos_info.values() for vid in channel_vids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(\"..\", \"data\", \"thumbnail-objects\")\n",
    "\n",
    "video_results_dir = os.path.join(os.path.join(\"..\",\"..\",\"DATA\",\"thumbnail-objects\",\"videos\"))\n",
    "channel_results_dir = os.path.join(RESULTS_DIR, \"channels\")\n",
    "def get_done_list(dir):\n",
    "    return [nm.replace(\".json\",'') for nm in os.listdir(dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del YOLOS_model\n",
    "torch.cuda.memory_allocated() / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del YOLOS_model\n",
    "except:\n",
    "    YOLOS_model = load_model()\n",
    "torch.cuda.memory_allocated() / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated() / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code in batches\n",
    "batch_size = 4\n",
    "batch_num = len(videos)//batch_size\n",
    "if batch_num != int(len(videos)/batch_size):\n",
    "    batch_num += 1 \n",
    "\n",
    "quality = ThumbnailURL.high\n",
    "\n",
    "done_list = get_done_list(video_results_dir)\n",
    "for batch in tqdm(range(batch_num)):\n",
    "\n",
    "    vid_batch = videos[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "    ids = [vid[\"id\"] for vid in vid_batch if\n",
    "        # os.path.isfile(\"../data/thumbnails/\"+vid[\"id\"]+\"_high.jpg\") and\n",
    "        vid[\"id\"] not in done_list]\n",
    "\n",
    "    if not ids:\n",
    "        continue\n",
    "\n",
    "    # imgs_paths = [\"../data/thumbnails/\"+vid_id+\"_high.jpg\" for vid_id in ids]\n",
    "    # images = [Image.open(path) for path in imgs_paths]\n",
    "\n",
    "    raws = []\n",
    "    fetched_ids = []\n",
    "    for id in ids:\n",
    "        url = thumbnail_URL(id, quality)\n",
    "        try:\n",
    "            raws.append(requests.get(url, stream=True).raw)\n",
    "        except:\n",
    "            continue\n",
    "        fetched_ids.append(id)\n",
    "    images = [Image.open(raw) for raw in raws]\n",
    "    ids = fetched_ids\n",
    "\n",
    "    # images = [resize(img, 100) for img in images]\n",
    "\n",
    "    results = model(images)\n",
    "\n",
    "    for vid_id, result in zip(ids, results):\n",
    "        path = os.path.join(video_results_dir, f\"{vid_id}.json\")\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make channel inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "channel_videos_dict = {}\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        channel_videos_dict.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make index\n",
    "for channel,videos in tqdm(channel_videos_dict.items()):\n",
    "    inv_idx = defaultdict(list)\n",
    "    for vid in videos:\n",
    "        try:\n",
    "            with open(os.path.join(video_results_dir, vid[\"id\"]+\".json\"), \"r\") as f:\n",
    "                objects = [o[\"pred_class\"] for o in json.load(f)]\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        for o in set(objects):\n",
    "            inv_idx[o].append(vid[\"id\"])\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"channels_inv_index\", f\"{channel}.json\"), \"w\") as f:\n",
    "        json.dump(inv_idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate channel results\n",
    "for channel,videos in tqdm(channel_videos_dict.items()):\n",
    "    video_info_dict = {vid[\"id\"]:vid for vid in videos}\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"channels_inv_index\", f\"{channel}.json\"), \"r\") as f:\n",
    "        inv_idx = json.load(f)\n",
    "\n",
    "    channel_result = {\n",
    "        \"object_views\": sort_dict({k:sum([video_info_dict[id][\"views\"] for id in v]) for k,v in inv_idx.items()}),\n",
    "        \"object_counts\": sort_dict({k:len(v) for k,v in inv_idx.items()}),\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(channel_results_dir, f\"{channel}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(channel_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(os.path.join(\"..\", \"data\", \"channel2category.json\"), \"r\") as f:\n",
    "    channel2cat = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of channels for each category\n",
    "category_channels_dict = {}\n",
    "for cat in tqdm(Topic._member_names_):\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        category_channels_dict[cat] = list(json.load(f).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make index\n",
    "for cat,channels in category_channels_dict.items():\n",
    "    print(cat)\n",
    "    \n",
    "    inv_indices = []\n",
    "    for channel in tqdm(channels):\n",
    "        with open(os.path.join(RESULTS_DIR, \"channels_inv_index\", f\"{channel}.json\"), \"r\") as f:\n",
    "            inv_indices.append(json.load(f))\n",
    "\n",
    "    inv_idx = extend_dicts(inv_indices)\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"categories_inv_index\", f\"{cat}.json\"), \"w\") as f:\n",
    "        json.dump(inv_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate category results\n",
    "for cat,channels in tqdm(category_channels_dict.items()):\n",
    "    video_info_dict = {vid[\"id\"]:vid for channel in channels for vid in channel_videos_dict[channel]}\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"categories_inv_index\", f\"{cat}.json\"), \"r\") as f:\n",
    "        inv_idx = json.load(f)\n",
    "\n",
    "    category_result = {\n",
    "        \"object_views\": sort_dict({k:sum([video_info_dict[id][\"views\"] for id in v]) for k,v in inv_idx.items()}),\n",
    "        \"object_counts\": sort_dict({k:len(v) for k,v in inv_idx.items()}),\n",
    "    }\n",
    "    \n",
    "    filepath = os.path.join(RESULTS_DIR, \"categories\", f\"{cat}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(category_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD*IDF for channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Frequency\n",
    "for cat,channels in category_channels_dict.items():\n",
    "    print(cat)\n",
    "\n",
    "    df = defaultdict(int)\n",
    "    for channel in tqdm(channels):\n",
    "        with open(os.path.join(RESULTS_DIR, \"channels_inv_index\", f\"{channel}.json\"), \"r\") as f:\n",
    "            inv_idx = json.load(f)\n",
    "        for token in inv_idx:\n",
    "            df[token] += 1\n",
    "\n",
    "    df = sort_dict(df)\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"categories_doc_freq\", f\"{cat}.json\"), \"w\") as f:\n",
    "        json.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf*idf\n",
    "for cat,channels in category_channels_dict.items():\n",
    "    print(cat)\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"categories_doc_freq\", f\"{cat}.json\"), \"r\") as f:\n",
    "        df = json.load(f)\n",
    "    idf = {k:np.log(len(channels)/v) for k,v in df.items()}\n",
    "\n",
    "    for channel in tqdm(channels):\n",
    "        with open(os.path.join(RESULTS_DIR, \"channels\", f\"{channel}.json\"), \"r\") as f:\n",
    "            tf = json.load(f)[\"object_counts\"]\n",
    "\n",
    "        terms = list(tf.keys())\n",
    "        freqs = np.array(list(tf.values()))\n",
    "        channel_idf = np.array([idf[k] for k in tf])\n",
    "\n",
    "        tf_idf = freqs*channel_idf\n",
    "        tf_idf = {term:freq for term,freq in zip(terms,tf_idf)}\n",
    "\n",
    "        tf_idf = sort_dict(tf_idf)\n",
    "\n",
    "        with open(os.path.join(RESULTS_DIR, \"channels_tf_idf\", f\"{channel}.json\"), \"w\") as f:\n",
    "            json.dump(tf_idf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF*IDF for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Frequency\n",
    "df = defaultdict(int)\n",
    "for cat,channels in category_channels_dict.items():\n",
    "    print(cat)\n",
    "\n",
    "    for channel in tqdm(channels):\n",
    "        with open(os.path.join(RESULTS_DIR, \"channels_inv_index\", f\"{channel}.json\"), \"r\") as f:\n",
    "            inv_idx = json.load(f)\n",
    "        for token in inv_idx:\n",
    "            df[token] += 1\n",
    "\n",
    "df = sort_dict(df)\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, f\"doc_freq_all.json\"), \"w\") as f:\n",
    "    json.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf*idf\n",
    "with open(os.path.join(RESULTS_DIR, f\"doc_freq_all.json\"), \"r\") as f:\n",
    "    df = json.load(f)\n",
    "idf = {k:np.log(len(channel2cat)/v) for k,v in df.items()}\n",
    "\n",
    "for cat,channels in category_channels_dict.items():\n",
    "    print(cat)\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"categories\", f\"{cat}.json\"), \"r\") as f:\n",
    "        tf = json.load(f)[\"object_counts\"]\n",
    "\n",
    "    terms = list(tf.keys())\n",
    "    freqs = np.array(list(tf.values()))\n",
    "    cat_idf = np.array([idf[k] for k in tf])\n",
    "\n",
    "    tf_idf = freqs*cat_idf\n",
    "    tf_idf = {term:freq for term,freq in zip(terms,tf_idf)}\n",
    "\n",
    "    tf_idf = sort_dict(tf_idf)\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"categories_tf_idf\", f\"{cat}.json\"), \"w\") as f:\n",
    "        json.dump(tf_idf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get broad idea of what is finished\n",
    "done_list = get_done_list(video_results_dir)\n",
    "channel_counts = defaultdict(int)\n",
    "for id in done_list:\n",
    "    channel_counts[vid2channel[id]] += 1\n",
    "cat_counts = defaultdict(int)\n",
    "for chan,count in channel_counts.items():\n",
    "    if count >= 30:\n",
    "        cat_counts[channel2cat[chan]] += 1\n",
    "cat_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b74cd7db1eb9f8499da7dbef20678a005a07ab79df7dd49707a224686fb33242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
