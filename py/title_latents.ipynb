{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from util.constants import Topic\n",
    "from util.helpers import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens').to(device)\n",
    "\n",
    "def model(titles):\n",
    "    return sbert_model.encode(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get latents for all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "channel_videos_dict = {}\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        channel_videos_dict.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(\"..\", \"data\", \"title-latents\")\n",
    "\n",
    "video_results_dir = os.path.join(\"..\",\"..\",\"DATA\",\"title-latents\",\"videos\")\n",
    "channel_results_dir = os.path.join(RESULTS_DIR, \"channels\")\n",
    "\n",
    "if not os.path.exists(video_results_dir):\n",
    "    os.makedirs(video_results_dir)\n",
    "\n",
    "def get_done_list(dir):\n",
    "    return [nm.replace(\".json\",'').replace('.pt','') for nm in os.listdir(dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code in batches\n",
    "for channel,videos in tqdm(channel_videos_dict.items()):\n",
    "    done_path = os.path.join(RESULTS_DIR, \"videos_done\", channel+\".json\")\n",
    "    ids = [vid[\"id\"] for vid in videos[:30]]\n",
    "    try:\n",
    "        with open(done_path, \"r\") as f:\n",
    "            done_ids = json.load(f)\n",
    "        if set(ids).issubset(done_ids): # Every id is done\n",
    "            continue\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    titles = [vid[\"title\"] for vid in videos[:30]]\n",
    "    \n",
    "    batch_latents = np.array(model(titles))\n",
    "\n",
    "    path = os.path.join(video_results_dir, f\"{channel}.pt\")\n",
    "    torch.save(batch_latents, path)\n",
    "    with open(done_path, \"w\") as f:\n",
    "        json.dump(ids, f)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "channel_videos_dict = {}\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        channel_videos_dict.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate channel results\n",
    "for channel,videos in tqdm(channel_videos_dict.items()):\n",
    "    in_path = os.path.join(video_results_dir,channel+\".pt\")\n",
    "    result_list = torch.load(in_path)\n",
    "\n",
    "    channel_mean = result_list.mean(axis=0)\n",
    "    channel_result = {\n",
    "        \"std\": float(result_list.std()),\n",
    "        \"len\": len(result_list),\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(channel_results_dir, f\"{channel}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(channel_result, f)\n",
    "    filepath = os.path.join(RESULTS_DIR, \"channels_mean\", f\"{channel}.pt\")\n",
    "    torch.save(channel_mean, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(os.path.join(\"..\", \"data\", \"channel2category.json\"), \"r\") as f:\n",
    "    channel2cat = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of results per channel for each category\n",
    "category_results_list = defaultdict(list)\n",
    "for channel in tqdm(get_done_list(channel_results_dir)):\n",
    "    cat = channel2cat[channel]\n",
    "    results = {}\n",
    "    for folder,ext in [(\"channels\",\".json\"), (\"channels_mean\",\".pt\")]:\n",
    "        filepath = os.path.join(RESULTS_DIR, folder, channel+ext)\n",
    "        try:\n",
    "            if ext == \".json\":\n",
    "                with open(filepath, \"r\") as f:\n",
    "                    results.update(json.load(f))\n",
    "            elif ext == \".pt\":\n",
    "                results.update({\"mean\": torch.load(filepath)})\n",
    "        except JSONDecodeError:\n",
    "            print(f\"couldn't open {channel}; deleting file\")\n",
    "            os.remove(filepath)\n",
    "    category_results_list[cat].append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate category results\n",
    "for cat,stats_list in category_results_list.items():\n",
    "    mean_list = np.array([channel_stats[\"mean\"] for channel_stats in stats_list])\n",
    "    std_list = np.array([channel_stats[\"std\"] for channel_stats in stats_list])\n",
    "    category_mean = mean_list.mean(axis=0)\n",
    "    category_result = {\n",
    "        \"std\": float(mean_list.std()),\n",
    "        \"avg_std_of_channels\": std_list.mean(),\n",
    "        \"len\": len(mean_list),\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(RESULTS_DIR, \"categories\", f\"{cat}.json\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(category_result, f)\n",
    "    filepath = os.path.join(RESULTS_DIR, \"categories_mean\", f\"{cat}.pt\")\n",
    "    torch.save(category_mean, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = get_done_list(channel_results_dir)\n",
    "\n",
    "channel_means = []\n",
    "for channel in tqdm(channels):\n",
    "    path = os.path.join(RESULTS_DIR, \"channels_mean\", channel+\".pt\")\n",
    "    channel_means.append(torch.load(path))\n",
    "channel_means = torch.Tensor(channel_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = Topic._member_names_\n",
    "\n",
    "category_means = []\n",
    "for cat in tqdm(categories):\n",
    "    path = os.path.join(RESULTS_DIR, \"categories_mean\", cat+\".pt\")\n",
    "    category_means.append(torch.load(path))\n",
    "category_means = torch.Tensor(category_means)\n",
    "\n",
    "all_means = torch.cat((channel_means, category_means), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(arrayA, arrayB):\n",
    "    \"\"\"[N,D], [M,D] -> [N,M]\"\"\"\n",
    "    normA = torch.norm(arrayA, p=2, dim=1)[:,None]\n",
    "    normB = torch.norm(arrayB, p=2, dim=1)[:,None]\n",
    "    return arrayA/normA @ (arrayB/normB).T\n",
    "\n",
    "def dist(tensorA, tensorB):\n",
    "    \"\"\"[N,D], [M,D] -> [N,M]\"\"\"\n",
    "    return ((tensorA[:,:,None] - tensorB.transpose()[None,:,:])**2).mean(1)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "all_means = all_means.to(device)\n",
    "best_similarity = torch.zeros(len(all_means)).to(device)\n",
    "best_repr = [\"\"]*len(all_means)\n",
    "\n",
    "batch_size = 128\n",
    "for i in tqdm(range(len(channels)//batch_size+1)):\n",
    "    done_ids = []\n",
    "    done_means = torch.zeros((0,768), device=device)\n",
    "    for channel in channels[i*batch_size:(i+1)*batch_size]:\n",
    "        done_path = os.path.join(RESULTS_DIR, \"videos_done\", channel+\".json\")\n",
    "        with open(done_path, \"r\") as f:\n",
    "            done_ids.extend(json.load(f))\n",
    "        if not done_ids:\n",
    "            continue\n",
    "        \n",
    "        filepath = os.path.join(video_results_dir, channel+\".pt\")\n",
    "        loaded_means = torch.Tensor(torch.load(filepath)).to(device)\n",
    "        done_means = torch.cat((done_means, loaded_means), dim=0)\n",
    "\n",
    "    if len(done_means) < 1:\n",
    "        continue\n",
    "\n",
    "    similarities = torch.max(cos_sim(all_means, done_means), dim=-1)\n",
    "    update_best = (similarities.values > best_similarity).nonzero()\n",
    "    best_similarity[update_best] = similarities.values[update_best]\n",
    "    for idx in update_best:\n",
    "        best_repr[idx] = done_ids[similarities.indices[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid2title = {}\n",
    "for cat in Topic._member_names_:\n",
    "    with open(os.path.join(\"..\", \"data\", \"info_videos\", F\"videos-info_{cat}.json\"), \"r\") as f:\n",
    "        videos_info = json.load(f)\n",
    "    vid2title.update({vid[\"id\"]:vid[\"title\"] for channel_vids in tqdm(videos_info.values()) for vid in channel_vids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_best_repr = {k:{\"sim\": sim.item(), \"vid_id\": id, \"repr\": vid2title[id]} for k,sim,id in zip(channels,best_similarity,best_repr) if id}\n",
    "with open(os.path.join(RESULTS_DIR, \"channels_best_repr.json\"), \"w\") as f:\n",
    "    json.dump(channels_best_repr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_best_repr = {k:{\"sim\": sim.item(), \"vid_id\": id, \"repr\": vid2title[id]} for k,sim,id in zip(categories,best_similarity[-9:],best_repr[-9:]) if id}\n",
    "with open(os.path.join(RESULTS_DIR, \"categories_best_repr.json\"), \"w\") as f:\n",
    "    json.dump(categories_best_repr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video latent deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(channels):\n",
    "    done_path = os.path.join(RESULTS_DIR, \"videos_done\", channel+\".json\")\n",
    "    with open(done_path, \"r\") as f:\n",
    "        done_ids = json.load(f)\n",
    "    if not done_ids:\n",
    "        continue\n",
    "    \n",
    "    filepath = os.path.join(video_results_dir, channel+\".pt\")\n",
    "    loaded_means = torch.load(filepath)\n",
    "    done_means = loaded_means\n",
    "\n",
    "    if len(done_means) < 1:\n",
    "        continue\n",
    "\n",
    "    distances = dist(channel2mean[channel][None], done_means)[0]\n",
    "\n",
    "    result = {\n",
    "        \"mean\": float(distances.mean()),\n",
    "        \"datapoints\": [{\n",
    "            \"x\": float(d),\n",
    "            \"y\": vid2views[id],\n",
    "            \"id\": id,\n",
    "            # \"title\": vid2title[id],\n",
    "        } for d,id in zip(distances, done_ids)]\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"video_deviation\", channel+\".json\"), \"w\") as f:\n",
    "        json.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b74cd7db1eb9f8499da7dbef20678a005a07ab79df7dd49707a224686fb33242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
